\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Inferencia variacional}
%La inferencia variacional busca aproximar una medida de probabilidad $p$ con la $q\in\mathcal{Q}$ que más se le parezca en cierto sentido. Formalmente, se elige la solución a la ecuación \eqref{eqn:vi} que reescribimos aquí
%
%\begin{equation*}
%	q^* = \argmin_{q \in \mathcal{Q}} D_{KL}(q\mmidp)
%\end{equation*}
%
%Para estudiar con detenimiento la función objetivo de la inferencia variacional, la construimos a partir de teoría de decisión con la doble intención de profundizar en la inferencia variacional y en el marco bayesiano. 

\section{Teoría de decisión}
\begin{definition}
	Un \textit{problema de decisión} es una tupla $(\Omega, \mathcal{C}, \mathcal{D}, \preceq)$ donde
	\begin{itemize}
		\item $\Omega$ es un $\sigma$-álgebra de eventos relevantes $\omega$
		\item $\mathcal{C}$ es un conjunto de consecuencias $c$
		\item $\mathcal{D}$ es un conjunto de decisiones y 
		\item $\prec$ es una relación binaria en $D$.
	\end{itemize}
\end{definition}
	
Esta definición intenta formalizar la experiencia de un individuo o grupo (a quien llamaremos tomadora de decisiones) que debe elegir una opción de $\mathcal{D}$ en contexto de incertidumbre. Ella tiene control sobre la decisión que toma, pero una vez elegida $d$ puede ocurrir cualquiera de los eventos $\omega$, y la combinación trae como consecuencia a $c$. La teoría bayesiana postula una serie de axiomas para resolver problemas de decisión sin caer en errores sistemáticos. Para una construcción detallada el lector puede referirse a \cite{bernardo}, pero las principales consecuencias que derivan de ellos son
\begin{enumerate}[label=\roman*]
	\item toda incertidumbre sobre $\Omega$ puede extraerse de $\prec$ y cuantificarse en una distribución de probabilidad $p(\omega)$
	\item toda preferencia debe cuantificarse en una \textit{función de utilidad} \\  $u: \mathcal{D}\times\Omega \to \mathbb{R}$  que captura la utilidad de haber tomado la decisión $d$ dado que ocurrió el evento   $\omega$ y
	\item la solución al problema de decisión consistente con los axiomas de coherencia es elegir la decisión que maximice la utilidad esperada.
	\item la manera en que la tomadora de decisiones debe actualizar sus creencias sobre $\omega$, dado que sucedió un evento $\delta$, es usando el teorema de Bayes \eqref{eqn:bayes-thm}
		\begin{equation*}
		p(\omega\mid\delta) = \frac{p(\delta\mid\omega)p(\omega)}{p(\delta)}
		\end{equation*}

\end{enumerate}

\begin{definition}
	La \textit{solución bayesiana} a un problema de decisión es 
	\begin{equation}
	d^* = \argmax_{d \in\mathcal{D}} \ \mathbb{E}_{\omega\sim p}\left[u(d, \omega)\right]
	\end{equation}
\end{definition}

La notación anterior es un poco abstracta, pero en el caso de datos $x$ generados por un modelo de probabilidad $p(x\mid\theta)$, si inicialmente la tomadora de decisiones cuantifica su incertidumbre sobre $\theta$ con $p(\theta)$, la manera de actualizar se escribe en la familiar forma

\begin{equation*}
	p(\theta | x ) 	\propto p(x|\theta)p(\theta).
\end{equation*}

En \cite{bernardo} se muestra que los problemas de estimación puntual, estimación por regiones y contraste de hipótesis pueden plantearse como problemas de decisión, tanto en contextos de inferencia pura como de pronóstico. Esta absoluta generalidad es importante, pues todo problema estadístico puede resolverse con el mismo camino: basta elegir distribuciones iniciales, una utilidad y maximizar.

Ahora consideraremos un problema de decisión particular que aunque suene un poco artificial, es de hecho la definición operativa\footnote{Es decir, que pueda utilizarse en el mundo real} que Bruno De Finetti usa para probabilidad en \cite{definetti2008}.  Supongamos que interesa cuantificar en una medida de probabilidad la incertidumbre que una tomadora de decisiones tiene acerca de un evento en el que debe ocurrir alguna posibilidad  $\omega \in \Omega$. Ella debe reportar una distribución para las $\omega$, pero nada la obliga a revelar sus verdaderas creencias. Si una vez ocurrido el evento de interés se le recompensará con una cantidad $u(\omega,p(\omega))$ que depende del evento que haya ocurrido y de las probabilidades que había expresado; ¿podrá la función $u$  asegurar que ella reporte sus verdaderas creencias?  Este problema se lo planteó Glenn Brier en 1959 para encontrar una manera de evaluar las predicciones meteorológicas, y la solución que le dio sigue recibiendo su nombre. 

\begin{definition}
	Consideremos un problema de decisión donde el conjunto de decisiones $\mathcal{Q}$ es el de distribuciones de probabilidad sobre ciertos eventos $\Omega$. A la utilidad $u: \mathcal{Q}\times\Omega \to \mathbb{R}$ se le llama \textit{función score}.
\end{definition}

Si la tomadora de decisiones siguiera los axiomas de coherencia, su respuesta sería reportar la distribución que maximice la utilidad esperada. Sin embargo, ella tiene una verdadera distribución de creencias, que denotaremos $p$. 

\begin{definition}
	Una función score $u$ es \textit{propia} si
	\begin{equation*}
		\sup_{Q \in \mathcal{q}} \mathbb{E}_{\omega\sim p}\left[u(Q,\omega)\right] = \mathbb{E}_{\omega\sim p}\left[u(p,\omega)\right]
	\end{equation*}
\end{definition}

Es decir, si la tomadora de decisiones maximiza su utilidad esperada reportando sus verdaderas creeencias. Brier probó que la pérdida cuadrática (o \textit{score de Brier}) es una función score propia.

Otra característica que puede incorporarse a la función score del problema, y que Bernardo argumenta es lo que debería hacerse en el conocimiento científico, es sólo evaluar la respuesta en términos de la probabilidad que asigna al evento que sí ocurrió. Esto permite ignorar lo poco acertada que pudo haber sido la tomadora de decisiones en algo que no ocurrió y por tanto no hay forma \enquote{honesta} o \enquote{justa} de evaluar. Como ejemplo para concretar, si se está modelando la probabilidad de que uno de tres eventos (digamos $A, B, C$) suceda y al final sucede $A$, se quiere recompensar a la tomadora de decisiones utilizando solamente la probabilidad que asignó a $A$, sin importar cuánto haya asignado a $B$ y $C$. 

\begin{definition}
	Una función score $u$ es \textit{local} si para cada $q \in \mathcal{Q}$ existen funciones $\{u_\omega: \omega\in\Omega\}$ tales que
	\begin{equation*}
		u(q, \omega) = u_\omega(q)
	\end{equation*}
\end{definition}

El siguiente teorema se prueba en \cite{bernardo} y caracteriza, bajo ciertas condiciones de regularidad, a las funciones score propias.

\begin{prop}
	Si $u:\mathcal{Q}\times\Omega \to \mathbb{R}$ es una función score propia, local y continuamente diferenciable tal que $\mathbb{E}_{\omega \sim p}\left[u(q,\omega)\right] < \infty$ para cada $q \in \mathcal{Q}$, entonces u debe ser de la forma
	\begin{equation*}
		u(q, \omega) = a\log q(\omega)+b_\omega
	\end{equation*}
\end{prop} 

\begin{definition}
A una función score que satisfaga las hipótesis de la proposición 2.1 se le llama \textit{score logarítmica}.	
\end{definition}

Ahora bien, la tomadora de decisiones puede encontrarse en una situación en la que, por dificultad computacional o cualquier otra razón no pueda reportar $p$, sino solamente una aproximación $q$. En este caso, la diferencia en utilidad esperada de la tomadora de decisiones es 

\begin{align*}
	\Delta &= \mathbb{E}_{\omega \sim p}\left[u(p, \omega)\right] - \mathbb{E}_{\omega\sim p}\left[u(q, \omega)\right]  \\
	&= \mathbb{E}_{\omega \sim p}\left[u(p, \omega) - u(q, \omega)\right] \\
	&= \mathbb{E}_{\omega \sim p}\left[(a\log p(\omega) - b_\omega) - (a\log q(\omega) - b_\omega) \right] \\
	&= a\mathbb{E}_{\omega \sim p}\left[\log\frac{p(\omega)}{q(\omega)}\right]
\end{align*}

\begin{definition}
	Cuando $a=1$, la expresión de arriba se llama \textit{divergencia de Kullback-Leibler} a $p$ desde $q$. Es decir, 
	\begin{equation}
	\kl(p \mmid q) = \mathbb{E}_{\omega \sim p}\left[\log\frac{p(\omega)}{q(\omega)}\right]	
	\end{equation}
\end{definition}

\section{La divergencia de Kullback-Leibler}
El término \textit{divergencia} es importante de resaltar porque, aunque la construcción recién hecha muestra que $\kl$ mide una noción de diferencia entre distribuciones de probabilidad; no es una métrica en el sentido estricto (no es simétrica ni satisface la desigualdad del triángulo). La simetría es fácil de imponer tomando $\skl(p, q) = \kl(p\mmid q) + \kl(q \mmid p)$, y esta cantidad es a la que Kullback llamaba \textit{divergencia}, pues prefería el término \enquote{información de discriminación} para la versión asimétrica. La definición (2.7) se ha mantenido porque la asimetría entre las distribuciones resulta particularmente útil. A continuación probaremos algunas propiedades de la divergencia de Kullback-Leibler, y para ello introducimos una definición más formal que depende de algunos resultados de teoría de la medida que enunciamos sin demostración.

\begin{definition}
Sean $(\Omega, \mathcal{F})$ un espacio medible y $P, Q$ dos medidas en $\Omega$. $P$ es \textit{absolutamente continua con respecto a} $Q$, denotado $P\ll Q$ si
	para todo $E\in\mathcal{F}$,  $Q(E)=0 \Rightarrow P(E)=0$.
\end{definition}

\begin{theorem}[Radon-Nikodym] \ \\
Sea $(\Omega, \mathcal{F})$  un espacio medible y $P, Q: \Omega \to \overline{\mathbb{R}}$ dos medidas $\sigma$-finitas en $\Omega$ tales que $P \ll Q$. Entonces, existe una función medible $f: \Omega \to \overline{\mathbb{R}}$ tal que para todo $E \in \mathcal{F}$
\begin{equation}
	P(E) = \int_E f dQ
\end{equation}
\end{theorem}
\begin{proof} \ \\
Ver \cite{bernardo}.	
\end{proof}


\begin{definition}
A la función $f$ en la ecuación (2.3) se le llama \textit{derivada de Radon-Nikodym de} $P$ \textit{con respecto a} $Q$, y se denota 
\begin{equation*}
\frac{dP}{dQ} := f
\end{equation*}
Cuando $P$ es una medida de probabilidad se usa el término \textit{densidad de probabilidad generalizada (f.d.p.g)}.
\end{definition}

\begin{prop}
	Sea $(\Omega, \mathcal{F})$ un espacio medible y $P, Q \ll L$ medidas $\sigma$-finitas en $\Omega$. 
	\begin{enumerate}[label=\roman*.]
		\item 
			\begin{equation*}
				\frac{d(P+Q)}{dL}=\frac{dP}{dL}+\frac{dQ}{dL}\quad L\text{-casi dondequiera}
			\end{equation*}
		\item Si $P\ll Q$
			\begin{equation*}
				\frac{dL}{dP} = \frac{dL}{dQ}\frac{dQ}{dP} \qquad L\text{-casi dondequiera}
			\end{equation*}
		\item Si $g$ es $P$-integrable,
			\begin{equation*}
				\int g dP=\int g \frac{dP}{dL}dL
			\end{equation*}
	\end{enumerate}
\end{prop}
\begin{proof} \ \\
Ver \cite{bernardo}.	
\end{proof}


\begin{definition}
Sea $(\Omega, \mathcal{F})$ un espacio de medible y $P, Q: \mathcal{F} \to [0,1]$ dos medidas de probabilidad tales que $P\ll Q$. La \textit{divergencia de Kullback-Leibler} a $Q$ desde $P$ es
\begin{equation}
	\kl(P\mmid Q) := \int\log\left(\frac{dP}{dQ}\right)dP
\end{equation}
\end{definition}

Si $p$ y $q$ son densidades de probabilidad, por definición $p=\frac{dP}{d\lambda}$ y $q=\frac{dQ}{d\lambda}$, donde $\lambda$ es la medida de Lebesgue. Entonces, podemos reescribir (2.4) como

\begin{equation*}
\kl(P\mmid Q) = \int\log\left(
		\frac{\frac{dP}{d\lambda}}{\frac{dQ}{d\lambda}}
	\right)
	\frac{d\lambda}{d\lambda}dP	
	= \int\log\left(\frac{p}{q}\right)pd\lambda
\end{equation*}
 
que es (2.2). Esta observación es de hecho más general, pues se satisface para cualquier medida $\mu$ tal que $P\ll Q \ll \mu$. 
\begin{theorem} \ \\
Sean $p_X, q_X$ densidades de probabilidad para una variable aleatoria $X$. $\kl$ es invariante bajo  transformaciones invertibles de $X$. Es decir, para toda función invertible $g$, si $Y=g(X)$ entonces $\kl(p_X\mmid q_X) = \kl(p_Y\mmid q_Y)$.

\end{theorem}

\begin{proof} \ \\
\begin{align*}
D_{KL}(p_Y\mmid q_Y) &= \int\log\left(\frac{p_Y}{q_Y}\right)p_Yd\lambda \\
&= \int \log\left(\frac{d\mu_Y}{d\nu_Y}\right)d\mu_Y \\
&= \int \log\left(\frac{d}{d}\right)\frac{d\mu_Y}{d\mu_X}d\mu_X
\end{align*}

\end{proof}




\begin{lemma}[Desigualdad log sum] \ \\
	Sean $a_1, \cdots, a_n$ y $b_1, \cdots, b_n \geq 0$. Si denotamos $a=\sum_ia_i$ y $b=\sum_ib_i$, entonces
	\begin{equation*}
	\sum_{i=1}^na_i\log\frac{a_i}{b_i} \geq a\log\frac{a}{b}	
	\end{equation*}
\end{lemma}
\begin{proof} \ \\
Sea $f(x)=x \log x$. 	
\begin{align*}
\sum_{i=1}^na_i\log\frac{a_i}{b_i} &= 
	\sum_{i=1}^nb_if\left(\frac{a_i}{b_i}\right) \\
	&= b\sum_{i=1}^n\frac{b_i}{b}f\left(\frac{a_i}{b_i}\right) \\
	&\geq bf\left(\sum_{i=1}^n\frac{b_i}{b}\frac{a_i}{b_i}\right)\\
	&= bf\left(\frac{a}{b}\right) \\
	&= a\log\frac{a}{b}
\end{align*}
La desigualdad se sigue de Jensen, pues $\sum_i\frac{b_i}{b}=1$ y $f$ es convexa, pues $f''(x)=\frac{1}{x}$.
\end{proof}


\begin{theorem}[Propiedades de $\kl$ -- caso general] \ \\
Consideremos una familia $\mathcal{F}$ de distribuciones de probabilidad para una variable aleatoria $X$ con densidades generalizadas $\{p\}$. Entonces,
\begin{enumerate}[label=\roman*.]

\item $\kl(p \mmid q) \geq 0$ para todas $p, q$ con igualdad si y sólo si $q \ll p$ y $p=q$ casi dondequiera.
\item $\kl$ es convexa en ambos argumentos
\item Para toda transformación invertible $X=g^{-1}(Y)$,
		\begin{equation*}
			\kl\left(p_Y\mmid q_Y\right) =
			\kl\left(p_X\mmid q_X\right) 
		\end{equation*}

\end{enumerate}
\end{theorem}
\begin{proof} \ \\
Nótese primero que por la desigualdad de Jensen,
\begin{align*}
-\kl(p\mmid q) 
	&= \mathbb{E}_{x\sim p}\left[\log\frac{q(x)}{p(x)}\right] \\
	&\leq\log\mathbb{E}_{x\sim p}\left[\frac{q(x)}{p(x)}\right] \\
	&= \log\int_\mathcal{X}qd\lambda\\
	&= \log 1 \\
	&= 0
\end{align*}

Como $-\log$ es estrictamente convexa, usar la desigualdad de Jensen lleva a la igualdad si y sólo si $p=q$ casi dondequiera.
La prueba de convexidad es estándar. Definamos
\begin{align*}
	p_\lambda(x) &= \lambda p_0(x) + (1-\lambda)p_1(x):=a_1+a_2 \\
	q_\lambda(x) &= \lambda q_0(x) + (1-\lambda)q_1(x):=b_1+b_2
\end{align*}
Entonces, 

\begin{align*}
\kl(p_\lambda\mmid q_\lambda) &= \int_\mathcal{X}p_\lambda(x)\log\frac{p_\lambda(x)}{q_\lambda(x)}dx \\
&=\int_\mathcal{X}(a_1+a_2)\log\frac{a_1+a_2}{b_1+b_2}dx \\
&\leq \int_\mathcal{X}a_1\log\frac{a_1}{b_1}+a_2\log\frac{a_2}{b_2}dx \\
&= \int_\mathcal{X}\lambda p_0(x)\log\frac{\lambda p_0(x)}{\lambda q_0(x)}dx +\int_\mathcal{X}(1-\lambda) p_1(x)\log\frac{(1-\lambda) p_1(x)}{(1-\lambda) q_1(x)}dx \\
&= \lambda \kl(p_0\mmid q_0) + \kl(p_1\mmid q_1)
\end{align*}

por lo que $\kl$ es convexa. \\

Por último, nótese que por el teorema de cambio de variable
\begin{equation*}
\frac{p_Y(y)}{q_Y(y)} =  \frac{p_X\left(g^{-1}(y)\right)\mid\frac{dg}{dy}\mid}{q_X\left(g^{-1}(y)\right)\mid\frac{dg}{dy}\mid} = 
\frac{p_X(x)}{q_X(x)}
\end{equation*}
\end{proof}




\end{document}