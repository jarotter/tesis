\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Introducción}
La estadística bayesiana interpreta la probabilidad no como un límite de frecuencias relativas en repeticiones independientes de un experimento, sino como la cuantificación de la incertidumbre personal. Así, es posible hablar de situaciones como \textquote{la probabilidad de que los demócratas ganen la elección en 2020} aunque sea imposible repetirlo. En el contexto de un modelo estadístico en el que un fenómeno $X$ tiene una medida de probabilidad parametrizada por $\theta \in \Theta$, esta diferencia epistemológica decanta en la asignación de una medida de probabilidad $p(\theta)$. La inferencia bayesiana se interpreta entonces como actualizar las creencias acerca de $\theta$ usando el teorema de Bayes

\begin{equation}
	p(\theta | x) = \frac{p(x|\theta)p(\theta)}{p(x)}
	\label{eqn:bayes-thm}	
\end{equation}

Al término del lado izquierdo se le llama \textit{probabilidad posterior}, y refleja el cambio en nuestra incertidumbre tras observar los datos $x$. Como extensión, el marco de referencia bayesiano permite hacer revisión de modelos de manera sencilla. Simulando de la \textit{posterior predictiva} 

\begin{equation*}
	p(x^*) = \int_\Theta p(x^*|\theta)p(\theta | x) d\theta	
\end{equation*}
 
 es posible comparar si el modelo propuesto coincide con los datos observados. Para Gelman y otros autores \cite{gelman-philosphy} este proceso es parte fundamental del análisis bayesiano, y permite evaluar la \textit{a priori} $p(\theta)$ pensada como parte del modelo. Sin ahondar más en esta discusión, la inferencia bayesiana es al menos una manera sistemática de cuantificar la incertidumbre y de dar resultados más sencillos de interpretar (por ejemplo, los intervalos creíbles sí son probabilidades, a diferencia de los intervalos de confianza).

Sin embargo, el denominador en \eqref{eqn:bayes-thm} es un cuello de botella computacional, pues es una integral complicada en dimensiones usualmente altas. En la práctica se utilizan algoritmos para muestrear de $p(\theta |x)$ sin conocer su forma explícita. Los \textit{métodos de Monte Carlo con cadenas de Markov} (MCMC por sus siglas en inglés y a partir de ahora) construyen una cadena de Markov ergódica cuya distribución estacionaria es $p(\theta | x)$, de manera que desde cualquier punto inicial pueda obtenerse una cadena que a la larga dé muestras de la posterior \cite{robert-book}. Estos algoritmos facilitaron el uso de procedimientos bayesianos, pero en modelos complicados, asegurar la convergencia es difícil y no escalan al tamaño masivo de los conjuntos de datos modernos.

Para mejorar la convergencia de MCMC, existen algoritmos que exploran $\Theta$ de manera más eficiente usando ideas de mecánica estadística \cite{betancourt-hmc}, pero siguen sin ser suficientemente rápidos para usarse en grandes volúmenes. Por otro lado, trabajo reciente de Broderick y Campbell \cite{coresets, coresets-greedy} propone métodos para construir \textit{coresets}, submuestras ponderadas que aproximan la verosimilitud de la totalidad de los datos, y efectivamente reducen el tamaño de muestra. Esta idea es perfectamente compatible con MCMC y con la clase de algoritmos que este trabajo estudiará: los variacionales.

Los algoritmos frecuentistas se benefician del descenso en gradiente estocástico que utiliza muestras pequeñas para estimar el gradiente de la función de pérdida y pueden así utilizarse en contextos masivos \cite{goodfellow}. Para poder usar esta ventaja en la formulación bayesiana, la inferencia debe traducirse en un problema de optimización. Como se verá con detenimiento en el siguiente capítulo, una formulación natural es aproximar la posterior $p$ con la distribución $q^*$ que minimice la divergencia de Kullback-Leibler de $q$ a $p$ en un conjunto $\mathcal{Q}$. 

\begin{equation}
	q^* = \argmin_{q \in \mathcal{Q}} D_{KL}(q || p)
	\label{eqn:vi}	
\end{equation}

Es importante notar que la calidad de la solución depende del espacio $\mathcal{Q}$, que tiene que ser suficientemente rico para aproximar fielmente a $p$ pero suficientemente estructurado para que el algoritmo converja en la práctica. El objetivo de este texto es introducir un algoritmo variacional no paramétrico propuesto por Liu y Wang \cite{svgd}, que que elige como $\mathcal{Q}$ un espacio de Hilbert con núcleo reproductor (EHNR a partir de ahora) y que puede interpretarse como descenso en gradiente (funcional) en este espacio.

{\color{red} ESTRUCTURA DEL DOCUMENTO}
\end{document}
