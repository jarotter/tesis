@article{kernelized-stein-discrepancy,
abstract = {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein's identity with the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.},
archivePrefix = {arXiv},
arxivId = {1602.03253},
author = {Liu, Qiang and Lee, Jason D. and Jordan, Michael I.},
eprint = {1602.03253},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/svgd/kernelized-stein-discrepancy.pdf:pdf},
number = {1},
title = {{A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation}},
url = {http://arxiv.org/abs/1602.03253},
year = {2016}
}

@article{svgd,
abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
archivePrefix = {arXiv},
arxivId = {1608.04471},
author = {Liu, Qiang and Wang, Dilin},
eprint = {1608.04471},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/svgd/svgd.pdf:pdf},
issn = {10495258},
pages = {4--7},
pmid = {18792489},
title = {{Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm}},
url = {http://arxiv.org/abs/1608.04471},
year = {2016}
}

@article{svgd-gradient-flow,
abstract = {Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics. We develop a geometric perspective that views SVGD as a gradient flow of the KL divergence functional under a new metric structure on the space of distributions induced by Stein operator.},
author = {Liu, Qiang},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/svgd/svgd-gradient-flow.pdf:pdf},
number = {Nips},
title = {{Stein variational gradient descent as gradient flow}},
url = {http://papers.nips.cc/paper/6904-stein-variational-gradient-descent-as-gradient-flow},
year = {2017}
}

@article{svgd-moment-matching,
abstract = {Stein variational gradient descent (SVGD) is a non-parametric inference algorithm that evolves a set of particles to fit a given distribution of interest. We analyze the non-asymptotic properties of SVGD, showing that there exists a set of functions, which we call the Stein matching set, whose expectations are exactly estimated by any set of particles that satisfies the fixed point equation of SVGD. This set is the image of Stein operator applied on the feature maps of the positive definite kernel used in SVGD. Our results provide a theoretical framework for analyzing the properties of SVGD with different kernels, shedding insight into optimal kernel choice. In particular, we show that SVGD with linear kernels yields exact estimation of means and variances on Gaussian distributions, while random Fourier features enable probabilistic bounds for distributional approximation. Our results offer a refreshing view of the classical inference problem as fitting Stein's identity or solving the Stein equation, which may motivate more efficient algorithms.},
archivePrefix = {arXiv},
arxivId = {1810.11693},
author = {Liu, Qiang and Wang, Dilin},
doi = {arXiv:1810.11693v1},
eprint = {1810.11693},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/svgd/svgd-moment-matching.pdf:pdf},
issn = {10495258},
pmid = {18792489},
title = {{Stein Variational Gradient Descent as Moment Matching}},
url = {http://arxiv.org/abs/1810.11693},
year = {2018}
}

@article{measuring-quality,
abstract = {Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein's method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.},
archivePrefix = {arXiv},
arxivId = {1703.01717},
author = {Gorham, Jackson and Mackey, Lester},
doi = {10.1016/j.bbr.2008.10.038},
eprint = {1703.01717},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/svgd/deeper/measuring-sample-quality.pdf:pdf},
isbn = {0022-0515},
issn = {1872-7549},
pmid = {19041347},
title = {{Measuring Sample Quality with Kernels}},
url = {http://arxiv.org/abs/1703.01717},
year = {2017}
}

@article{vi,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1601.00670v9},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
doi = {10.1080/01621459.2017.1285773},
eprint = {arXiv:1601.00670v9},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/svgd/deeper/inferencia-variacional.pdf:pdf},
isbn = {1601.00670},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Algorithms,Computationally intensive methods,Statistical computing},
number = {518},
pages = {859--877},
pmid = {303902},
title = {{Variational Inference: A Review for Statisticians}},
volume = {112},
year = {2017}
}

@article{random-features-kernel,
author = {Rahimi, Ali and Recht, Ben},
doi = {10.1007/s12204-009-0467-7},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/svgd/deeper/random-features-kernel.pdf:pdf},
isbn = {160560352X},
issn = {10071172},
pmid = {4519940},
title = {{Random Features for Large-Scale Kernel Machines}}
}

@article{advi,
abstract = {Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use.},
archivePrefix = {arXiv},
arxivId = {1603.00788},
author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
doi = {10.3847/0004-637X/819/1/50},
eprint = {1603.00788},
file = {:Users/jarotter/Documents/ITAM/8 - Oto{\~{n}}o 2018/proyecto{\_}simulacion/docs/1603.00788.pdf:pdf},
isbn = {1603.00788},
issn = {15337928},
pages = {1--38},
title = {{Automatic Differentiation Variational Inference}},
url = {http://arxiv.org/abs/1603.00788},
year = {2016}
}

@book{rkhs-book,
author = {Berlinet, Alain and Thomas-Agnan, Christine},
doi = {10.1142/9789812835635_0014},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/rkhs/Alain Berlinet, Christine Thomas-Agnan - Reproducing Kernel Hilbert Spaces in Probability and Statistics (2004, Springer).pdf:pdf},
isbn = {9781461347927},
pages = {153--162},
publisher = {Springer Science + Business Media},
title = {{Reproducing Kernels in Probability and Statistics}},
year = {2009}
}


@article{bayesian-gradient-flow,
abstract = {The Bayesian update can be viewed as a variational problem by characterizing the posterior as the minimizer of a functional. The variational viewpoint is far from new and is at the heart of popular methods for posterior approximation. However, some of its consequences seem largely unexplored. We focus on the following one: defining the posterior as the minimizer of a functional gives a natural path towards the posterior by moving in the direction of steepest descent of the functional. This idea is made precise through the theory of gradient flows, allowing to bring new tools to the study of Bayesian models and algorithms. Since the posterior may be characterized as the minimizer of different functionals, several variational formulations may be considered. We study three of them and their three associated gradient flows. We show that, in all cases, the rate of convergence of the flows to the posterior can be bounded by the geodesic convexity of the functional to be minimized. Each gradient flow naturally suggests a nonlinear diffusion with the posterior as invariant distribution. These diffusions may be discretized to build proposals for Markov chain Monte Carlo (MCMC) algorithms. By construction, the diffusions are guaranteed to satisfy a certain optimality condition, and rates of convergence are given by the convexity of the functionals. We use this observation to propose a criterion for the choice of metric in Riemannian MCMC methods.},
archivePrefix = {arXiv},
arxivId = {1705.07382},
author = {Trillos, Nicolas Garcia and Sanz-Alonso, Daniel},
eprint = {1705.07382},
file = {:Users/jarotter/Documents/ITAM/tesis/refs/povi/bayesian-gradient-flow.pdf:pdf},
keywords = {convexity,gradient flows,riemannian mcmc,wasserstein space},
number = {0},
pages = {1--30},
title = {{The Bayesian update: variational formulations and gradient flows}},
url = {http://arxiv.org/abs/1705.07382},
year = {2017}
}

@article{advances-vi,
abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully used in various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
archivePrefix = {arXiv},
arxivId = {1711.05597},
author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
doi = {10.1109/TPAMI.2018.2889774},
eprint = {1711.05597},
file = {:Users/jarotter/Documents/ITAM/tesis/docs/vi/advances-variational-inference.pdf:pdf},
isbn = {03051838},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Approximate Bayesian Inference,Bayes methods,Computational modeling,Hidden Markov models,Inference Networks,Market research,Optimization,Probabilistic logic,Reparameterization Gradients,Scalable Inference,Stochastic processes,Structured Variational Approximations,Variational Inference},
pages = {1--23},
pmid = {30596568},
title = {{Advances in Variational Inference}},
year = {2018}
}

@book{robert-book,
address = {New York},
author = {Robert, Christian P. and Casella, George},
edition = {Second},
file = {:Users/jarotter/Documents/estadistica/bayesiana/libros/MCSM casella.pdf:pdf},
isbn = {0-387-21239-6},
publisher = {Springer Science + Business Media},
title = {{Monte Carlo Statistical Methods}},
year = {2004}
}
@article{esl,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hastie},
doi = {10.1007/b94608},
eprint = {arXiv:1011.1669v3},
file = {:Users/jarotter/Documents/Books/ESLII{\_}print10.pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
journal = {The Mathematical Intelligencer},
number = {2},
pages = {83--85},
pmid = {15512507},
title = {{Springer Series in Statistics The Elements of}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
volume = {27},
year = {2009}
}

@article{goodfellow,
abstract = {Deep Learning book},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Ian Goodfellow, Yoshua Bengio}, Aaron Courville},
doi = {10.1016/B978-0-12-391420-0.09987-X},
eprint = {arXiv:1011.1669v3},
file = {:Users/jarotter/Documents/data/deep{\_}learning/[Goodfellow]{\_}deep{\_}learning.pdf:pdf},
isbn = {3540620583, 9783540620587},
issn = {1432122X},
journal = {MIT Press},
number = {7553},
pages = {785},
pmid = {21728107},
title = {{The Deep Learning Book}},
volume = {521},
year = {2017}
}

@article{betancourt-hmc,
archivePrefix = {arXiv},
arxivId = {arXiv:1701.02434v2},
author = {Betancourt, Michael},
eprint = {arXiv:1701.02434v2},
file = {:Users/jarotter/Documents/estadistica/bayesiana/mcmc/hamiltonian-mcmc.pdf:pdf},
title = {{A Conceptual Introduction to Hamiltonian Monte Carlo}}
}

@article{coresets,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.05053v2},
author = {Campbell, Trevor and Broderick, Tamara},
eprint = {arXiv:1710.05053v2},
file = {:Users/jarotter/Documents/estadistica/bayesiana/coresets/coresets.pdf:pdf},
pages = {1--36},
title = {{Automated scalable bayesian inference via hilbert coresets}},
year = {2013}
}
@article{coresets-greedy,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.01737v2},
author = {Campbell, Trevor and Broderick, Tamara},
eprint = {arXiv:1802.01737v2},
file = {:Users/jarotter/Documents/estadistica/bayesiana/coresets/coreset-geodesic-ascent.pdf:pdf},
title = {{Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent}},
year = {2018}
}


